\documentclass[12pt, preprint]{aastex}
\usepackage{graphicx}	% For figures
\usepackage{natbib}	% For citep and citep
\usepackage{amsmath}	% for \iint
\usepackage{bm}
\usepackage[breaklinks]{hyperref}	% for blackboard bold numbers
\usepackage{hyperref}
\hypersetup{colorlinks}
\usepackage{color}
\usepackage{morefloats}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkblue}{rgb}{0,0,0.5}
\hypersetup{ colorlinks,
linkcolor=darkblue,
filecolor=darkgreen,
urlcolor=darkred,
citecolor=darkblue }

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\vep}{\bm{\epsilon}}
\newcommand{\ep}{\epsilon}


\begin{document}

\title{Likelihood free inference of the halo model parameters with simulations of galaxy mock catalogs}

\begin{abstract}

The Halo Occupation Distribution (HOD) model is a powerful prescription of how galaxies populate 
dark matter halos that has had much success in reproducing large scale galaxy clustering and understanding
galaxy evolution within the context of the halo model. Constraints on the parameters of the HOD model have 
so far been infered using a likelihood function that assumes, \emph{First} a functional (often times Gaussian) form of the likelihood, 
\emph{Second} analytical model for the galaxy clustering statistics. We use Approximate Bayesian Computation (ABC) 
in order to infer the HOD parameters by assuming a prior over the parameters and a distance function between 
statistical summaries of the data and statistical summaries of the simulations.
Using a mock galaxy catalog created with known HOD parameters, we obtain precise constraints on the HOD parameters that 
reproduces the summary statistics of the mock galaxy catalog accurately. 
Since ABC does not require computing the likelihood, we include observables beyond galaxy clustering such as 
the galaxy number density and the galaxy group richness, for which accurately constructing a likelihood is intractable. 
We demonstrate that adding these measurements to our set of observables leads to stronger constraints on the HOD parameters, 
specifically the ones that govern the satellite galaxy population. Moreover, the constraints we infer that 
reproduce the known HOD parameters illustrate the success of ABC in the context of constraining HOD parameters. 

\end{abstract}

\section{Introduction}

form of the likelihood. It is often assumed that the likelihood is Gaussian distribution 
with a given covariance matrix that needs to be evaluated from simulating a large set of 
mock catalogs, or other techniques such as bootstrap. In order to 
compute the likelihood funciton, one needs to invert a high dimensional covariance 
matrix which makes the likelihood evaluation and therefore sampling from the posterior 
computationally intractable.

Approximate Bayesian Computation, ABC, is a method that eliminates the need for likelihood. 
In recent years, this technique has been used in a number of cosmological studies, such as 
the inference of cosmological parameters from modelling of the distances of supernovae type Ia, 
and modelling of weak lensing peak maps. Exploring the parameter space with ABC algorithm 
requires three essential ingredients: (1) a prior pdf on the parameters that we wish to estimate, 
(2) a forward model (simulator) of the observations, and (3) a distance metric for comparing 
the model and the observed data. In problems where likelihood evaluation is not analytically or 
computationally tractable, this algorithm bypasses the need for likelihood evaluation.

To date, different variations of ABC algorithm have been introduced  and implemented 
in the literature. One of the earliest methods is likelihood free Markov Chain Monte 
Carlo. In this algorithm, an initial draw from the prior is kept, if the distance between 
the simulated model corresponding to that parameter and the observed data is less than a threshold. 
In the consequent steps, the proposed parameters are accepted if \emph{first} they satisfy the 
distance requirement, and \emph{second} the Metropolis-Hasting (MH) acceptance ratio evaluated for the 
proposal is less than one. In the MH acceptance ratio, the likelihood is replaced with some transition 
kernel. 

Based on importance sampling, several modifications to this algorithm have been proposed. 
These methods are called ABC Sequential Monte Carlo (ABC-SMC), or ABC Population Monte Carlo 
(ABC-PMC). These algorithms rely on sampling a large set of parameters from the prior pdf, and assigning 
equal weights to them as an initial step. In the subsequent steps, sampling is modified by 
proposing intermediate distribution built from the previous sample of parameters and their 
corresponding weights. Parameters are updated by sampling from the intermediate distribution 
and are accepted if they pass the distance requirements. Furthermore, weights are updated according 
to the intermediate distribution. 

Talk about importance sampling here ...
Sampling from the parameter space in ABC algorithm can become more efficient by 
employing adaptive importance sampling... 

\section{Method}

\subsection{Approximate Bayesian Computation}

ABC is a likelihood free method for inference of a set of parameters that are required for the simulation of a 
physical system, through a forward model, a distance function, and a prior probability distribution over the parameters. 
Let us denote the the data by $\mathcal{D}$. We assume that there exist a forward model $\mathcal{M}$ that 
produces a simulation of the data $\bar{\mathcal{D}}$ for a given set of parameters $\bm{\theta}$
\beq
\bar{\mathcal{D}}_{\theta} = \mathcal{M}(\bm{\theta})
\eeq

Furthermore, ABC requires adopting a distance function between the data $\mathcal{D}$ and the simulation $\bar{\mathcal{D}}_{\theta}$.
The distance function, in general, can be a vector $\bm{d}(\mathcal{D},\bar{\mathcal{D}}_{\theta})$
\begin{eqnarray}
\bm{d}(\mathcal{D},\bar{\mathcal{D}}_{\theta}) &=& [d_{1,\theta}, ... , d_{M,\theta}], \\
d_{i,\theta} &=& d_{i}(\mathcal{S}_{i}(\mathcal{D}), \mathcal{S}_{i}(\bar{\mathcal{D}}_{\theta}))  ~~\mbox{for $i=\{1,...,M\}$},
\end{eqnarray}
where $d_{i}(\dot,\dot)$ is a scalar function that defines a distance 
between $i$-$th$ summary statistics of the data $\mathcal{S}_{i}(\mathcal{D})$ and the same summary statistics of 
the simulation $\mathcal{S}_{i}(\bar{\mathcal{D}}_{\theta})$, and $M$ is the total number of statistical summaries 
used in the analysis.

ABC algorithm draws a large number of samples from the prior distribution over parameter space, 
and rejects every sample for which the distance between the data and the simulation corresponding to that 
sample is not less than a certain threshold $\bm{\epsilon}$. This requirement is satisfied \emph{if and only if}
every component of the distance vector is less than its corresponding threshold. Note that, each summary statistics 
has a separate threshold. This procedure is deone iteratively untill, an investigator is left with a certain number 
of samples that satisfy the distance criteria. The final ensemble approximates the posterior pdf over 
the paramters of the forward model of the data. This sampling method is called \emph{rejected sampling}.

In practice however, rejected sampling is not desirable because $(i)$ it requires fine tuning of the threshold 
$\bm{\epsilon}$ whose elements are small, and $(ii)$ finding a large number of samples that pass the threshold 
criteria is computationally expensive. Alternatively, we draw a large number of proposals from the prior pdf over 
the HOD parameters with the only requirement that they pass the distance criterion. But since we select an 
arbitrarily large initial threhold, all th proposals pass the test. Moreover, we assign simillar weights to all 
the proposals in the initial pool. In the next steps, we draw new proposals from the previos pool through 
weighted sampling, and we perturb the particles by a kernel that is twice the covariance matrix of the proposals 
in the previous pool.   

blaalalalal

\subsection{Halo Occupation Distribution}

Halo Occupation distribution is a powerful tool for description of galaxy/halo relation.


\section{Summary statistics}

\section{Forward modeling of the galaxy population with HALOTOOLS}


\section{Mock data}

We use HALOTOOLS to populate halos with galaxies above a certain luminosity threshold $M_r>-20$.


\section{Results}

Hello World!

\begin{thebibliography}{70}


\end{thebibliography}


\end{document}
