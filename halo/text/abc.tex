\documentclass[12pt, preprint]{aastex}
\usepackage{graphicx}	% For figures
\usepackage{natbib}	% For citep and citep
\usepackage{amsmath}	% for \iint
\usepackage{bm}
\usepackage[breaklinks]{hyperref}	% for blackboard bold numbers
\usepackage{hyperref}
\hypersetup{colorlinks}
\usepackage{color}
\usepackage{morefloats}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkblue}{rgb}{0,0,0.5}
\hypersetup{ colorlinks,
linkcolor=darkblue,
filecolor=darkgreen,
urlcolor=darkred,
citecolor=darkblue }

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\vep}{\bm{\epsilon}}
\newcommand{\ep}{\epsilon}


\begin{document}

\title{Likelihood free inference of the halo model parameters with group and clustering 
       statistics of the simulations of galaxy mock catalogs}

\begin{abstract}

The Halo Occupation Distribution model is a powerful prescription of how galaxies populate 
dark matter halos that has had much success in reproducing large scale galaxy clustering and understanding
galaxy evolution within the context of the halo model. Constraints on the parameters of the HOD model have 
so far been infered using a likelihood function that assumes, first a functional---Gaussian---form of the likelihood, 
and Second, an analytical model for the galaxy clustering statistics. We use Approximate Bayesian Computation 
in order to infer the HOD parameters by assuming a prior over the parameters, and a distance function between 
statistical summaries of the data and those of the simulations.
Using a mock galaxy catalog created with known HOD parameters, we obtain precise constraints on the HOD parameters that 
reproduces the summary statistics of the mock galaxy catalog accurately. 
Since ABC does not require computing the likelihood, we include observables beyond galaxy clustering such as 
the galaxy number density and the galaxy group multiplicity function. 
We demonstrate that adding these measurements leads to stronger constraints on the HOD parameters, specifically the parameters 
that control the population of the satellite galaxies.

\end{abstract}

\section{Introduction}

Hogg writes here...

Inference of the cosmological parameters and understanding the formation of galaxies within the context of large scale 
structure, requires understanding of the Galaxy-Halo connection. Halos are biased tracers of the large scale structure 
and galaxies reside dark matter Halos. Halo occupation distribution---HOD---is a probabilistic framework that describes how 
galaxies populate dark matter halos. The central tenet of HOD modeling is that the probability of $N$ galaxies residing a 
dark matter halo is determined by the mass of the host halo. This statistical prescription for connection of galaxies and halos 
has been successful in reproducing the galaxy two-point clustring as well as the galaxy-matter cross correlation---galaxy-galaxy 
lensing---within the context of the halo model, and has been used in understanding teh co-evolution of galaxies and halos and putting 
constraints on th relation between the stellar mass and halo mass of galaxies.

In the context of large scale clustering of galaxies, inference of the cosmological and halo model parameters are performed assuming 
an analytical model for the two-point correlation function of the galaxies, and a gaussian likehood. This Gaussian likelihood approximates 
the probability of observing the data---observed positions of the galaxies in the configuration space---given the halo model by computing 
the squared difference of the two-point correlation function measured from the observations and the analytical model for the galaxy 
clustering, weighted by a covariance matrix. The weighted difference of the number density of the observed galaxies and the analytical 
number density is often added to this. 

Hogg writes why we do not want to do this .....


Approximate bayesian computation (ABC) provides a framework that alleviates the issues arising from approximating the likelihood of 
observing the large scale population of galaxies given the cosmological and halo model parameters. ABC approximates the posterior 
probability distribution function over the model parameters given the data---the angular positions of the galaxies on the sky and 
their redshifts---by drawing proposals from the prior over the model parameters, simulating the data with the proposals, and rejecting 
the proposals for which the distance between the data and the simulation (measured by a metric) is less than a certain threshold. The 
procedure described here is rejected sampling. In practice rejected sampling is expensive and instead, investigators change start 
by sampling from the prior with a large ..........



\section{Method}

\subsection{Approximate Bayesian Computation}

ABC is a likelihood free method for inference of a set of parameters that are required for the simulation of a 
physical system, through a forward model, a distance function, and a prior probability distribution over the parameters. 
Let us denote the the data by $\mathcal{D}$. We assume that there exist a forward model $\mathcal{M}$ that 
simulates the data $\bar{\mathcal{D}}$ for a given set of parameters $\bm{\theta}$
\beq
\bar{\mathcal{D}}_{\theta} = \mathcal{M}(\bm{\theta})
\eeq

Furthermore, ABC requires adopting a distance function between the data $\mathcal{D}$ and the simulation $\bar{\mathcal{D}}_{\theta}$.
The distance function, in general, can be a vector $\bm{d}(\mathcal{D},\bar{\mathcal{D}}_{\theta})$
\begin{eqnarray}
\bm{d}(\mathcal{D},\bar{\mathcal{D}}_{\theta}) &=& [d_{1,\theta}, ... , d_{M,\theta}], \\
d_{i,\theta} &=& d_{i}(\mathcal{S}_{i}(\mathcal{D}), \mathcal{S}_{i}(\bar{\mathcal{D}}_{\theta}))  ~~\mbox{for $i=\{1,...,M\}$},
\end{eqnarray}
where $d_{i}(\dot,\dot)$ is a scalar function that defines a distance 
between $i$-$th$ summary statistics of the data $\mathcal{S}_{i}(\mathcal{D})$ and the same summary statistics of 
the simulation $\mathcal{S}_{i}(\bar{\mathcal{D}}_{\theta})$, and $M$ is the total number of statistical summaries 
used in the analysis.

We draw a large number of proposals from the prior distribution over the parameter space, 
and reject the samples for which the distance between the data and the simulation corresponding to that 
sample is more than a certain threshold $\bm{\epsilon}$. This requirement is satisfied \emph{if and only if}
every component of the distance vector is less than its corresponding threshold. Note that, each summary statistics 
has a separate threshold. This procedure is done iteratively untill, we are left with a certain number
of samples that satisfy the distance criteria. The final ensemble approximates the posterior pdf over 
the paramters of the forward model of the data. This sampling method is called \emph{rejected sampling}.

Recected sampling is expensive. Instead we use population monte carlo, adaptive importance sampling, ....

\subsection{Halo Model}

Halo Occupation distribution is a powerful tool for description of galaxy/halo relation.

...

\subsection{Summary statistics}

Talk about 2pcf, group multiplicity function, and number density ...

\section{Forward modeling of the galaxy population with HALOTOOLS}

Hello World!
Talk about why it is better to simulate the mock and compute the summary statistics of the mock instead of 
using analytical formulas...

Talk about all the assumptions that go into analytic formulas...

Talk about why we do not want to write down likelihood function for gmf, 2pcf, ...

...


\section{Mock data}

Hello World!

Plots showing 2pcf, gmf, ... of the mock data.
How we compute the covariance matrix from mocks...
Talk about error bars. Talk about poisson errors in group populations ...

\section{Results}

Show the posteriors...
Show evolution of the posterior errorbars with iteration. Show epsilon. Convice that we are converging ...
Draw samples from the posterior, populate the mock catalogs with those samples, and compute the summary statistcs with them 
and demonstrate that they match the data...

\section{Discussion}

talk about cosmological infeence and why this is good first step for likelihood free inference in large scale ...

talk about why it is hard to do cosmology ...

talk about halo model ...

talk about HOD and why it is too simple to be the right model ...

talk about assembly bias perhaps ... and how we can use the same machinery for detection of assemblt bias ... 

talk about writing down a likelihood for gmf, tpcf, ... , and the gaussianity assumption ....

talk about are results ...


talk about future directions ...

\begin{thebibliography}{70}


\end{thebibliography}


\end{document}
