\documentclass[12pt, preprint]{aastex}
\usepackage[breaklinks,colorlinks, urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{graphicx}	% For figures
\usepackage{natbib}	% For citep and citep
\usepackage{amsmath}	% for \iint
\usepackage{bm}
\usepackage[breaklinks]{hyperref}	% for blackboard bold numbers
\usepackage{hyperref}
\hypersetup{colorlinks}
\usepackage{color}
\usepackage{morefloats}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkblue}{rgb}{0,0,0.5}
\hypersetup{ colorlinks,
linkcolor=darkblue,
filecolor=darkgreen,
urlcolor=darkred,
citecolor=darkblue }

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\todo}[1]{{\bf \textcolor{red}{ #1}}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\lang}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\vep}{\bm{\epsilon}}
\newcommand{\ep}{\epsilon}


\begin{document}

\title{Likelihood free inference of the halo model parameters with group and clustering 
       statistics of the simulations of galaxy mock catalogs}

\begin{abstract}
{\bf other abstract for reference}\\
The Halo Occupation Distribution (HOD) model is a powerful prescription of how galaxies populate dark matter halos that has had much success in reproducing large scale galaxy clustering and understanding galaxy evolution within the context of the halo model. Constraints on the parameters of the HOD model have so far been imposed using the standard practice of Bayesian inference with likelihood estimation that assumes a functional (often times Gaussian) form of the likelihood. Likelihood free inference methods such as the Approximate Bayesian Computation (ABC) makes Bayesian inference possible without ever having to calculate the likelihood. 

In this work, we present ABC in the context of constraining the parameters of the HOD model. For a mock galaxy catalog with known HOD parameters, we use ABC on measurements of a set of observables generated through {\it Halotools}, a publicly available ..., to infer tight constraints that reproduce the known HOD paramters. Since ABC does not require computing the likelihood, we include observables beyond galaxy clustering such as the galaxy number density and the galaxy group richness, for which accurately constructing a likelihood is intractable. We demonstrate that adding these measurements to our set of observables leads to stronger constraints on the HOD parameters, 
 specifically the ones that govern the satellite galaxy population. Moreover, the constraints we infer that reproduce the known HOD parameters illustrate the success of ABC in the context of constraining HOD parameters.



The Halo Occupation Distribution model is a powerful prescription of how galaxies populate 
dark matter halos that has had much success in reproducing large scale galaxy clustering and understanding
galaxy evolution within the context of the halo model. Constraints on the parameters of the HOD model have 
so far been inferred using a likelihood function that assumes a functional--Gaussian--form of the likelihood
and an analytic model for the galaxy clustering statistics. Likelihood free inference methods such as the Approximate Bayesian Computation (ABC) makes Bayesian inference possible without ever having to calculate the likelihood. 

In this work, we use ABC in order to infer the HOD parameters by assuming a prior over the parameters, and a distance function between 
statistical summaries of the data and those of the simulations.
Using a mock galaxy catalog created with known HOD parameters, we obtain precise constraints on the HOD parameters that 
reproduces the summary statistics of the mock galaxy catalog accurately. 
Since ABC does not require computing the likelihood, we include observable beyond galaxy clustering such as 
the galaxy number density and the galaxy group multiplicity function. 
We demonstrate that adding these measurements leads to stronger constraints on the HOD parameters, specifically the parameters 
that control the population of the satellite galaxies.

\todo{Hogg writes here too}

\end{abstract}

\section{Introduction}

\todo{Hogg writes here ...}

Inference of the cosmological parameters and understanding the formation of galaxies within the context of large scale 
structure, requires understanding of the Galaxy-Halo connection. Halos are biased tracers of the large scale structure 
and galaxies reside in dark matter Halos. Halo occupation distribution---HOD---is a probabilistic framework that describes how galaxies populate dark matter halos. The central tenet of halo occupation model is that the probability that a certain number of galaxies populate a dark matter halo is determined by the mass of the host halo. This statistical prescription for connection of galaxies and halos has been successful in reproducing the galaxy two-point clustering as well as the galaxy-matter cross correlation---galaxy-galaxy 
lensing---within the context of the halo model, and has been used in understanding the co-evolution of galaxies and halos and putting constraints on the relation between the stellar mass and halo mass of galaxies.

In the context of large scale clustering of galaxies, inference of the cosmological and halo model parameters are performed assuming an analytic model for the two-point correlation function of the galaxies, and a Gaussian likelihood. The accuracy of the analytic model of the galaxy clustering statistics hinges upon the accuracy of the fitting functions such as halo bias, halo mass function that are accurate to ten percent level. There are other simplifying assumptions encoded in these analytic models such as the scale independence of the halo bias. 

Furthermore, the likelihood of observing the data---observed positions of the galaxies in the configuration space---given the cosmological and halo model parameters is approximated by a Gaussian function whose argument is the squared difference of the two-point correlation function measured from the observations and the analytic model for the galaxy clustering, weighted by a covariance matrix. The weighted difference of the number density of the observed galaxies and the analytic number density is also added to this. In this work, we investigate the how accurately we can infer the model parameters by relaxing the underlying assumptions of the parameter inference methods. 

Approximate bayesian computation (ABC) provides a \emph{rejected sampling} framework framework that alleviates the issues arising from approximating the likelihood of observing the large scale population of galaxies given the cosmological and halo model parameters. ABC approximates the posterior probability distribution function over the model parameters given the data---the angular positions of the galaxies on the sky and their redshifts---by drawing proposals from the prior over the model parameters, simulating the data with the proposals, and rejecting the proposals for which the distance between the data and the simulation (measured by a metric) is less than a certain small threshold. 

Furthermore, This sampling formalism permits us to sample the posterior of the model by simulating the data via a forward generative model that accounts for all the complications and uncertainties associated with the survey data that are difficult to take into account in approximating the likelihood function. In the context of the large galaxy surveys, these complications include masks, nontrivial survey geometry, and missing data as a result of fiber collision.

Rejected sampling in ABC requires a distance function that quantifies the closeness of the simulation (e.g. fiber collided galaxies) to the data (observed positions of the galaxies in the survey). In principle, this distance metric is a positive definite function  that compares various summary statistics of the data and those of the simulations. In this investigation, we focus on the two-point correlation, number density, and the group statistics (more specifically the abundance of the galaxy group richnesses). 

Performing a full cosmological inference using this sampling method requires a simulator that, (I) takes a set of cosmological parameters as input and generates a snapshot of dark matter particles at a given redshift using an N-body simulation, (II) builds a catalog of dark matter halos for that snapshot of the N-body simulation using a halo finder algorithm, and (III) paints galaxies into host halos given a set of halo occupation model parameters. 

Computationally, this process is extremely demanding. One way to make this problem more tractable is to split this into to separate this problem into two parts: \emph{emulation}, and \emph{simulation}. That is, one can use a set of N-body simulations with known cosmologies, and perform halo occupation simulations using the pre-built halo catalogs of those simulations. In order to compute the summary statistics corresponding to the samples from the parameter space for which there is no N-body simulation, one can interpolate between the summary statistics of the parameters for which we have available N-body simulations. This procedure is called emulation. Emulation has been introduced in the literature in coyote universe [CITE COYOTE].

In this work however, we narrow our focus on inferring the posterior probability over the parameters of the halo model using an already existing halo catalog of a dark matter particle N-body simulation with a fixed cosmology. That is we simplify the problem by leaving the problem of emulation for future investigations.

In practice, this rejected sampling method is replaced by a more efficient procedure. Instead, we start by sampling from the prior with a relatively large threshold. In the consequent steps, we iteratively perform importance sampling from the previous samples, and lower the threshold adaptively. 

This method requires a simulator or a forward model of the positions of galaxies given the model parameters. We use the publicly available code Halotools in order to populate the halo catalogs with galaxies brighter than a certain luminosity threshold.

\section{Method}

\subsection{Approximate Bayesian Computation}

ABC is a likelihood free method for inference of a set of parameters that are required for the simulation of a 
physical system, through a forward model, a distance function, and a prior probability distribution over the parameters. 
Let us denote the the data by $\mathcal{D}$. We assume that there exist a forward model $\mathcal{M}$ that 
simulates the data $\bar{\mathcal{D}}$ for a given set of parameters $\bm{\theta}$
\beq
\bar{\mathcal{D}}_{\theta} = \mathcal{M}(\bm{\theta})
\eeq

Furthermore, ABC requires adopting a distance function between the data $\mathcal{D}$ and the simulation $\bar{\mathcal{D}}_{\theta}$.
The distance function, in general, can be a vector $\bm{d}(\mathcal{D},\bar{\mathcal{D}}_{\theta})$
\begin{eqnarray}
\bm{d}(\mathcal{D},\bar{\mathcal{D}}_{\theta}) &=& [d_{1,\theta}, ... , d_{M,\theta}], \\
d_{i,\theta} &=& d_{i}(\mathcal{S}_{i}(\mathcal{D}), \mathcal{S}_{i}(\bar{\mathcal{D}}_{\theta}))  ~~\mbox{for $i=\{1,...,M\}$},
\end{eqnarray}
where $d_{i}(\dot,\dot)$ is a scalar function that defines a distance 
between $i$-$th$ summary statistics of the data $\mathcal{S}_{i}(\mathcal{D})$ and the same summary statistics of 
the simulation $\mathcal{S}_{i}(\bar{\mathcal{D}}_{\theta})$, and $M$ is the total number of statistical summaries 
used in the analysis.

We draw a large number of proposals from the prior distribution over the parameter space, 
and reject the samples for which the distance between the data and the simulation corresponding to that 
sample is more than a certain threshold $\bm{\epsilon}$. This requirement is satisfied \emph{if and only if}
every component of the distance vector is less than its corresponding threshold. Note that, each summary statistics 
has a separate threshold. This procedure is done iteratively untill, we are left with a certain number
of samples that satisfy the distance criteria. The final ensemble approximates the posterior probability distribution over the parameters of the generative model of the data. This sampling method is called \emph{rejected sampling}.

Rejected sampling is expensive. Instead we use Population Monte Carlo, adaptive importance sampling, ....

\subsection{Halo Model}

Halo Occupation distribution is a powerful tool for description of galaxy/halo relation. \todo{Here we explain HOD modeling of galaxy positions, etc.}

...

\subsection{Summary statistics}
\begin{itemize}

\item \todo{Talk about the observables: 2pcf, group multiplicity function, and number density ...}

\end{itemize}

\section{Forward modeling of the galaxy population with HALOTOOLS}

\begin{itemize}
\item \todo{Halotools allows us to forward model galaxy positions from HOD parameters. From the galaxy positions, we can calculate observable summary statistics such as the 2PCF, Group Multiplicity Function (GMF) and the number density $\bar{n}$.}

\end{itemize}
Talk about why it is better to simulate the mock and compute the summary statistics of the mock instead of 
using analytic formulas...

Talk about all the assumptions that go into analytic formulas...

Talk about why we do not want to write down likelihood function for gmf, 2pcf, ...

...


\section{Mock data}
Plots showing 2pcf, gmf, ... of the mock data.
How we compute the covariance matrix from mocks...
Talk about error bars. Talk about poisson errors in group populations ... 

\section{Results}
Show the posteriors...
Show evolution of the posterior errorbars with iteration. Show epsilon. Convice that we are converging ...
Draw samples from the posterior, populate the mock catalogs with those samples, and compute the summary statistcs with them 
and demonstrate that they match the data...

\subsection{Comparison to MCMC}
\todo{Apples to apples comparison between ABC and MCMC.}
\begin{itemize}
\item ABC-PMC using nbar and xi vs MCMC
\cite{Ishida:2015aa}
\end{itemize}

\subsection{ABC Including GMF}
ABC-PMC results using observables beyond galaxy clustering and nbar(z) GMF

\section{Discussion}
\todo{talk about cosmological infeence and why this is good first step for likelihood free inference in large scale ...}

\todo{talk about why it is hard to do cosmology ...}

talk about halo model ...

talk about HOD and why it is too simple to be the right model ...

talk about assembly bias perhaps ... and how we can use the same machinery for detection of assemblt bias ... 

talk about writing down a likelihood for gmf, tpcf, ... , and the gaussianity assumption ....

talk about are results ...


talk about future directions ...
\bibliographystyle{yahapj}
\bibliography{ccppabc}
\end{document}

